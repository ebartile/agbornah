import time
import random
import requests
import multiprocessing
from random import choice
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

USER_AGENTS = [
'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1.1 Safari/605.1.15',
'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:77.0) Gecko/20100101 Firefox/77.0',
'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36',
'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:77.0) Gecko/20100101 Firefox/77.0',
'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36',
'Mozilla / 5.0 (Macintosh; Intel Mac OS X 10_8_4) AppleWebKit / 536.30.1 (KHTML, like Gecko) Version / 6.0.5 Safari / 536.30.1',
'Mozilla / 5.0 (Macintosh; Intel Mac OS X 10_8_3) AppleWebKit / 536.29.13 (KHTML, like Gecko) Version / 6.0.4 Safari / 536.29.13',
'Mozilla / 5.0 (Macintosh; Intel Mac OS X 10_6_8) AppleWebKit / 534.57.2 (KHTML, like Gecko) Version / 5.1.7 Safari / 534.57.2',
'Mozilla / 5.0 (Macintosh; Intel Mac OS X 10_8_2) AppleWebKit / 536.26.17 (KHTML, like Gecko) Version / 6.0.2 Safari / 536.26.17',
'Mozilla / 5.0 (Macintosh; U; PPC Mac OS X; en) AppleWebKit / 124 (KHTML, like Gecko) Safari / 125',
]

USED_PROXIES = []
MAX_PROCESSES = 2

def get_proxy():
    while True:
        try:
            res = requests.get('https://free-proxy-list.net/',
                               headers={'User-Agent': 'Mozilla/5.0'})
            soup = BeautifulSoup(res.text, "html.parser")
            proxy_list = list()
            for items in soup.select("#list > div > div.table-responsive > div > table > tbody > tr"):
                proxy_list.append(':'.join([item.text for item in items.select("td")[:2]]))
            return proxy_list
        except Exception:
            continue

def run_test(proxy, results):
    options = Options()
    options.add_argument('--no-sandbox')
    options.add_argument('--headless')
    options.add_argument('user-agent=' + choice(USER_AGENTS))
    options.add_argument(f'--proxy-server:{proxy}')
    options.add_experimental_option('excludeSwitches', ['enable-logging'])
    driver = webdriver.Remote(
        options=options,
        command_executor='http://localhost:4444/wd/hub'
    )
    driver.implicitly_wait(30)

    try:
        with open("links.txt", "r") as file:
            for line in file:
                driver.get(line)
                time.sleep(3)
                wait = WebDriverWait(driver, 10)
                wait.until(EC.presence_of_element_located((By.XPATH, '//*[@id="songsDetails"]/div/article[1]/div/section[2]/div[2]/button[1]'))).click()
                time.sleep(35)
    finally:
        driver.quit()
        results.put(proxy)

if __name__ == "__main__":
    while True:
        processes = []
        results = multiprocessing.Queue()

        # Start up to MAX_PROCESSES processes
        for proxy in get_proxy():
            if len(processes) >= MAX_PROCESSES:
                break
            if proxy not in USED_PROXIES:
                USED_PROXIES.append(proxy)
                print(proxy)
                process = multiprocessing.Process(target=run_test, args=(proxy, results))
                processes.append(process)
                process.start()

        # Wait for any process to finish
        finished_proxy = results.get()

        # Remove the finished proxy from USED_PROXIES
        # USED_PROXIES.remove(finished_proxy)

        # Wait for all other processes to finish
        for process in processes:
            process.join()

        # time.sleep(60)
